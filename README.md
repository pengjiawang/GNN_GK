# GNN_GK
GNN_GK
- [Graph-Kernel-based-on-Graph-Nueral-Network](#Graph-Kernel-based-on-Graph-Nueral-Network)
  - [DKD](#dkd)
  - [GKD](#gkd)
  - [SKD](#skd)
- [Citation](#citation)



## DKD
|Method|Title|Link|Time|
|:---:|:---:|:---:|:---:|
|Kernel NN |Deriving neural architectures from sequence and graph kernels |[Paper](http://proceedings.mlr.press/v70/lei17a/lei17a.pdf) |2017 PMLR|
|Pre-DGCN |Pre-training graph neural networks with kernels |[Paper](https://arxiv.org/pdf/1811.06930.pdf) |2018 NIPS|
|KCNN |Kernel graph convolutional neural networks |[Paper](https://pubmed.ncbi.nlm.nih.gov/34061738/) |2018 Springer|
|RWNN |Random walk graph neural networks |[Paper](https://proceedings.neurips.cc/paper/2020/file/ba95d78a7c942571185308775a97a3a0-Paper.pdf) |2020 NeurIPS |
|GKNN |Graph kernel neural networks  |[Paper](https://arxiv.org/pdf/2112.07436.pdf) |2021|
|VES |Graph kernels combined with the neural network on protein classification |[Paper](https://www.worldscientific.com/doi/epdf/10.1142/S0219720019500306) |2019 World Scientific|
|CKGCN |Graph Neural Networks with Composite Kernels |[Paper](https://arxiv.org/pdf/2005.07869.pdf) |2020|
|GMKEA |Graph neural networks with multiple kernel ensemble attention |[Paper](https://www.sciencedirect.com/science/article/abs/pii/S095070512100561X) |2021 Elsevier|
|GCKN |Convolutional kernel networks for graph-structured data |[Paper](http://proceedings.mlr.press/v119/chen20h/chen20h.pdf) |2020 PMLR|
|GSKN |Theoretically improving graph neural networks via anonymous walk graph kernels |[Paper](https://arxiv.org/pdf/2104.02995.pdf) | 2020|
|TGNN |TGNN: A joint semi-supervised framework for graph-level classification |[Paper](https://arxiv.org/pdf/2304.11688.pdf) |2023|
|GraphHeat |Graph convolutional networks using heat kernel for semi-supervised learning |[Paper](https://arxiv.org/pdf/2007.16002.pdf) |2020|
|KerGNNs |Kergnns: Interpretable graph neural networks with graph kernels |[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/20615) |2022|
|Twin-GNN |Twin Weisfeiler-Lehman: High Expressive GNNs for Graph Classification |[Paper](https://arxiv.org/pdf/2203.11683.pdf) |2022|
|FusedMM |Fusedmm: A unified sddmm-spmm kernel for graph embedding and graph neural networks |[Paper](https://arxiv.org/pdf/2011.06391.pdf) |2021 IEEE|
|ESSPMM |Accelerating spmm kernel with cache-first edge sampling for graph neural networks |[Paper](https://arxiv.org/pdf/2104.10716.pdf) |2021|
|GKAT |Graph Consistency based Mean-Teaching for Unsupervised Domain Adaptive Person Re-Identification |[Paper](https://arxiv.org/pdf/2105.04776.pdf) |2021 IJCAI|
|GNTK |Saliency Prediction with External Knowledge |[Paper](https://ieeexplore.ieee.org/document/9423113) |2021 WACV|
|Node with GNTK |Learning student networks via feature embedding |[Paper](https://ieeexplore.ieee.org/document/9007474) |2020 TNNLS|
|Residuals GNTK |Inter-Region Affinity Distillation for Road Marking Segmentation |[Paper](https://ieeexplore.ieee.org/document/9156309) |2020 CVPR|
|Fast GNTK | Relational knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/8954416) |2019 CVPR|
|WNTK |Correlation congruence for knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/9009071) |2019 ICCV|
|KP-GNN|Similarity-preserving knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/9010328) |2019 ICCV|
|AKGNN |KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation |[Paper](https://arxiv.org/abs/2105.04181) |2021 IJCAI|
|RobustGCN |Tree-like Decision Distillation |[Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.pdf) |2021 CVPR|
|GCMNK |DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning |[Paper](https://www.zhuanzhi.ai/paper/12c6a095c207373b5e207d3375290234) |2022 WSDM|
|MKGCN |Conditional Graph Attention Networks for Distilling and Refining Knowledge Graphs in Recommendation |[Paper](https://dl.acm.org/doi/10.1145/3459637.3482331) |2021 CIKM|
|GBKGNN |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|BKGNN |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|KW-GCN |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|KGAT |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|KMAE |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|DHGK |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
|MLMKDNN |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR|
## GKD
|Method|Title|Link|Time|
|:---:|:---:|:---:|:---:|
|HIRE | HIRE: Distilling high-order relational knowledge from heterogeneous graph neural networks|[Paper](https://www.sciencedirect.com/science/article/pii/S0925231222009961) |2022 Neurocomputing|
|GFKD| Graph-Free Knowledge Distillation for Graph Neural Networksâˆ— |[Paper](https://arxiv.org/pdf/2105.07519.pdf)|2021 IJCAI |
|LWC-KD |Graph Structure Aware Contrastive Knowledge Distillation for Incremental Learning in Recommender Systems |[Paper](https://dl.acm.org/doi/10.1145/3459637.3482117) | 2021 CIKM|
|EGAD |EGAD: Evolving Graph Representation Learning with Self-Attention and Knowledge Distillation for Live Video Streaming Events |[Paper](https://ieeexplore.ieee.org/document/9378219) |2020 IEEE International Conference on Big Data|
|GRL |Graph Representation Learning via Multi-task Knowledge Distillation |[Paper](https://arxiv.org/pdf/1911.05700.pdf) |2019 NeurIPS Workshop |
|GFL |Graph Few-shot Learning via Knowledge Transfer |[Paper](https://arxiv.org/pdf/1910.03053.pdf) |2019 AAAI|
|HGKT |Heterogeneous Graph-based Knowledge Transfer for Generalized Zero-shot Learning |[Paper](https://ieeexplore.ieee.org/document/9412524) |2019 ICPR |
|AGNN |Amalgamating Knowledge from Heterogeneous Graph Neural Networks |[Paper](https://ieeexplore.ieee.org/document/9577636) |2021 CVPR |
|CPF |Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework |[Paper](https://dl.acm.org/doi/10.1145/3442381.3450068) |2021 WWW|
|LSP |Distilling Knowledge from Graph Convolutional Networks |[Paper](https://ieeexplore.ieee.org/document/9156492) |2020 CVPR|
|GKD |GKD:Semi-supervised Graph Knowledge Distillation for Graph-Independent Inference |[Paper](https://arxiv.org/abs/2104.03597) |2021 MICCAI |
|scGCN |scGCN is a graph convolutional networks algorithm for knowledge transfer in single cell omics |[Paper](https://www.nature.com/articles/s41467-021-24172-y.pdf) |2021 Nature |
|MetaHG |Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media |[Paper](https://papers.nips.cc/paper/2021/file/e234e195f3789f05483378c397db1cb5-Paper.pdf) | 2021 NeurIPS|
|Cold Brew | Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods|[Paper](https://arxiv.org/pdf/2111.04840.pdf) |2021 ICLR |
|PGD |Privileged Graph Distillation for Cold Start Recommendation |[Paper](https://arxiv.org/pdf/2105.14975.pdf) |2021 SIGIR |
|GLNN | Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation|[Paper](https://arxiv.org/pdf/2110.08727.pdf) |2022 ICLR |
|Distill2Vec |Distill2Vec: Dynamic Graph Representation Learning with Knowledge Distillation |[Paper](https://ieeexplore.ieee.org/document/9381315) |2020 ASONAM |
|MT -GCN |Mutual Teaching for Graph Convolutional Networks |[Paper](https://arxiv.org/pdf/2009.00952.pdf) |2021 FGCS |
|RDD |Reliable Data Distillation on Graph Convolutional Network |[Paper](https://dl.acm.org/doi/10.1145/3318464.3389706) |2020 SIGMOD |
|TinyGNN|TinyGNN: Learning efficient graph neural networks |[Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403236) |2020 KDD |
|GLocalKD|Deep Graph-level Anomaly Detection by Glocal Knowledge Distillation |[Paper](https://arxiv.org/abs/2112.10063) |2022 WSDM |
|OAD |Online Adversarial Distillation for Graph Neural Networks |[Paper](https://arxiv.org/abs/2112.13966) |2021 |
|SCR |SCR: Training Graph Neural Networks with Consistency Regularization |[Paper](https://arxiv.org/abs/2112.04319v2) |2021 |
|ROD|ROD: Reception-aware Online Distillation for Sparse Graphs |[Paper](https://dl.acm.org/doi/pdf/10.1145/3447548.3467221) |2021 KDD|
|EGNN |EGNN: Constructing explainable graph neural networks via knowledge distillation |[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0950705122001289) |2022 KBS|
|CKD |Collaborative Knowledge Distillation for Heterogeneous Information Network Embedding |[Paper](https://zhoushengisnoob.github.io/papers/WWW2022.pdf) |2022 WWW|
|G-CRD |On Representation Knowledge Distillation for Graph Neural Networks |[Paper](https://arxiv.org/pdf/2111.04964.pdf) | 2021|
|BGNN |Binary Graph Neural Networks |[Paper](https://ieeexplore.ieee.org/document/9578443) |2021 CVPR|
|EGSC |Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation|[Paper](https://papers.nips.cc/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf) |2021 NIPS |
|HSKDM |A graph neural network-based node classification model on class-imbalanced graph data |[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0950705122002374?via%3Dihub) | 2022 KBS|
|MustaD | Compressing deep graph convolution network with multi-staged knowledge distillation|[Paper](https://pdfs.semanticscholar.org/1069/51f925eaf30eee7ba738c9b89e665cfb2f22.pdf?_ga=2.172565539.160323435.1657536794-2050460118.1606810570) |2021 PloS one |


## SKD
|Method|Title|Link|Time|
|:---:|:---:|:---:|:---:|
|LinkDist |Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages |[Paper](https://arxiv.org/pdf/2106.08541.pdf) |2021 |
|IGSD |Iterative Graph Self-distillation |[Paper](https://arxiv.org/pdf/2010.12609.pdf) |2020 The Workshop on Self-Supervised Learning for the Web|
|GNN-SD |On Self-Distilling Graph Neural Network |[Paper](https://arxiv.org/pdf/2011.02255.pdf) |2020 IJCAI|
|SDSS|Multi-task Self-distillation for Graph-based Semi-Supervised Learning |[Paper](https://arxiv.org/pdf/2112.01174.pdf) |2021|
|SAIL |SAIL: Self-Augmented Graph Contrastive Learning |[Paper](https://arxiv.org/pdf/2009.00934.pdf) |2022 AAAI |

# Citation

        @article{liu2023graph,
          title={Graph-based Knowledge Distillation: A survey and experimental evaluation},
          author={Liu, Jing and Zheng, Tongya and Zhang, Guanzheng and Hao, Qinfen},
          journal={arXiv preprint arXiv:2302.14643},
          year={2023}
        }
